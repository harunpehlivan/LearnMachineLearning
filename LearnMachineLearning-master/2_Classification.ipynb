{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2.Classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPXGfqbmV8B3zz8/9AH971s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KhanradCoder/LearnMachineLearning/blob/master/2_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAXIzTTAMVXw",
        "colab_type": "text"
      },
      "source": [
        "# Preparing the data\n",
        "\n",
        "Here we are going to be using machine learning to predict breast cancer diagnosis."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDJnRjd7UJDj",
        "colab_type": "code",
        "outputId": "4e928df4-28f6-4064-93c2-b4c340779062",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataset = pd.read_csv('cancer.csv')\n",
        "print(len(dataset.columns))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qtTlILKMhBK",
        "colab_type": "text"
      },
      "source": [
        "We know we have a lot of x features, so we print the number of columns to see how many x features we need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIfaMwA1Dxql",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = dataset.iloc[:, 2:29].values\n",
        "y = dataset.iloc[:, 1].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6TZqI5TM0yd",
        "colab_type": "text"
      },
      "source": [
        "Now let's split the data into training and testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9nZXq5hEQpp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tfdGqBttM69g",
        "colab_type": "text"
      },
      "source": [
        "Here we perform feature scaling. Because we have a lot of features with different ranges for values, feature scaling will make our correlations more pronounced for our algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGxUDiGwEg5i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_test = scaler.transform(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4mT2elbVT5M",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression\n",
        "Logistic regression is the simplest classification algorithm (even though it has regression in the name). ![alt text](https://miro.medium.com/max/1428/1*Vd9ZTC1zWJPtV7iXPMJk1Q.png)\n",
        "You can see here that for predicting whether y=1 or 0 (a classification problem), linear regression performs poorly in comparison to logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LpvuQzQEtwB",
        "colab_type": "code",
        "outputId": "efa4c45d-3378-48f1-f937-f992219ed261",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logistic_classifier = LogisticRegression()\n",
        "logistic_classifier.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
              "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
              "                   multi_class='auto', n_jobs=None, penalty='l2',\n",
              "                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                   warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ZnnfxptEwoP",
        "colab_type": "code",
        "outputId": "275e1af7-1005-4cde-f402-be2f0b4d2b53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "y_preds = logistic_classifier.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test, y_preds))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[73  0]\n",
            " [ 3 38]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbIQ-1bpqGRG",
        "colab_type": "text"
      },
      "source": [
        "Here we print the confusion matrix, which shows how accurate our model is.\n",
        "![alt text](https://miro.medium.com/max/712/1*Z54JgbS4DUwWSknhDCvNTQ.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBpq4yG0VYmJ",
        "colab_type": "text"
      },
      "source": [
        "#  Support Vector Machines\n",
        "\n",
        "SVM's are among the most powerful machine learning algorithms. Put simply, the SVM algorithm tries to form a line that keeps the maximum space between a given number of clusters.\n",
        "\n",
        "\n",
        "![alt text](https://www.aitrends.com/wp-content/uploads/2018/01/1-19SVM-2.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaJV6vMpVh_O",
        "colab_type": "code",
        "outputId": "d056fe29-c8dc-431a-dc5e-035c89134acc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svm = SVC(kernel=\"rbf\")\n",
        "svm.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='rbf',\n",
              "    max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHqMVBOM_A4C",
        "colab_type": "code",
        "outputId": "c60caa95-1c69-427b-8de8-34f0b1b4f73a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "y_preds = svm.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test, y_preds))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[72  1]\n",
            " [ 2 39]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4tJBtYYVqx3",
        "colab_type": "text"
      },
      "source": [
        "# Decision Trees\n",
        "Decision trees are really common in machine learning. Essentially, the algorithm tries to find a set of rules by which it can classify each datapoint into a given category. See the example below of a decision tree that classifies whether or not a given person is \"fit\". ![alt text](https://www.tutorialspoint.com/machine_learning_with_python/images/decision_tree_introduction.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7en6qkvLVqkp",
        "colab_type": "code",
        "outputId": "42c0815a-cfd8-4ea2-fc7b-76fbe3a893f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "tree = DecisionTreeClassifier(criterion = 'entropy')\n",
        "tree.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n",
              "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
              "                       random_state=None, splitter='best')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xY_NdAogBrGB",
        "colab_type": "code",
        "outputId": "4c0e56e2-8f26-4bf0-a988-619634c01ce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "y_preds = tree.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test, y_preds))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[70  3]\n",
            " [ 3 38]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DS2h6EMViRP",
        "colab_type": "text"
      },
      "source": [
        "# Random Forest\n",
        "\n",
        "One tree is usually a pretty weak classifier. However, if you have multiple trees and average out their predictions, your classifier becomes a lot stronger. Think about examples of collective intelligence in nature; the more predictions, the more accurate on average our classifier should be. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGW33g4taiOe",
        "colab_type": "code",
        "outputId": "583c52d3-a04b-4fb9-8ff0-0ebab0bbe0e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        }
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "forest = RandomForestClassifier(n_estimators = 100, criterion = 'entropy')\n",
        "forest.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='entropy', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=100,\n",
              "                       n_jobs=None, oob_score=False, random_state=None,\n",
              "                       verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mus9ovUCIWg",
        "colab_type": "code",
        "outputId": "8474fe42-c05b-4399-c5f1-4e27bd7f65ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "y_preds = forest.predict(x_test)\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "print(confusion_matrix(y_test, y_preds))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[71  2]\n",
            " [ 3 38]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bDmb29DO-WT",
        "colab_type": "text"
      },
      "source": [
        "We can see that the difference between one decision tree and 100 decision trees is marginal. However, as you come across larger and more complex datasets, the number of trees will become more important. Remember though, decision trees are prone to overfit, especially on a small dataset like the one we are using."
      ]
    }
  ]
}